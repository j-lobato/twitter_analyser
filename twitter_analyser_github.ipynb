{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUyRA6FS5jrd"
   },
   "outputs": [],
   "source": [
    "# Add your Twitter credentials. Do NOT share them with anyone\n",
    "\n",
    "\n",
    "consumer_key =\n",
    "consumer_secret =\n",
    "access_token = \n",
    "access_token_secret ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3krE3w8kdDFK"
   },
   "outputs": [],
   "source": [
    "#! pip install tweepy\n",
    "\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth =1800\n",
    "\n",
    "# keys\n",
    "\n",
    "consumer_key = consumer_key\n",
    "consumer_secret = consumer_secret \n",
    "access_token = access_token\n",
    "access_token_secret = access_token_secret \n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth) \n",
    "\n",
    "\n",
    "\n",
    "#Switching to application authentication\n",
    "\n",
    "auth=tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "\n",
    "\n",
    "#Setting up new api wrapper, using authentication only\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True) \n",
    "\n",
    "if (not api):\n",
    "    print('problem with api')\n",
    "\n",
    "\n",
    "\n",
    "listOfTweets = []\n",
    "\n",
    "\n",
    "\n",
    "def query_twitter(keyword, lang, number_tweets):\n",
    "    for tweet in tweepy.Cursor(api.search, q=keyword, tweet_mode='extended', lang=lang).items(number_tweets):\n",
    "        # Add tweets in this format\n",
    "        dict_ = {'Screen Name': tweet.user.screen_name,\n",
    "          'User Name': tweet.user.name,\n",
    "          'Followers Count': tweet.user.followers_count, \n",
    "          'Friends Count': tweet.user.friends_count,\n",
    "          'Users Tweets Count': tweet.user.statuses_count,\n",
    "          'User Location': tweet.user.location,\n",
    "          'Coordinates': tweet.coordinates,\n",
    "          'Place': tweet.place, \n",
    "          'Tweet Created At': tweet.created_at,\n",
    "          'Tweet Text': tweet.full_text,\n",
    "          'Tweet Coordinates': tweet.coordinates,\n",
    "          'Retweet Count': tweet.retweet_count,\n",
    "          'Retweeted': tweet.retweeted,\n",
    "          'Phone Type': tweet.source,\n",
    "          'Favorite Count': tweet.favorite_count,\n",
    "          'Favorited': tweet.favorited,\n",
    "          'Replied': tweet.in_reply_to_status_id_str\n",
    "          }\n",
    "        listOfTweets.append(dict_)\n",
    "        df_tweets = pd.DataFrame(listOfTweets)\n",
    "    return df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnTOWiz3enKO"
   },
   "outputs": [],
   "source": [
    "# Type the following\n",
    "\n",
    "keyword = '' # type in between the '' in red \n",
    "\n",
    "lang = 'en' # en for English. \n",
    "\n",
    "number_tweets= 60 # type the number of tweets. The current limit is 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vc8sM3wi7YGj"
   },
   "outputs": [],
   "source": [
    "d = query_twitter (keyword =keyword, lang = lang, number_tweets=number_tweets)\n",
    "d = d.drop_duplicates(subset='Tweet Text')\n",
    "d.to_csv('dataframe.csv') \n",
    "\n",
    "# Delete the # below if you want to save the tables as CSV\n",
    "\n",
    "retweet_count10 = d.sort_values(by='Retweet Count').head(10).copy()\n",
    "\n",
    "#retweet_count10.to_csv('retweet_count10.csv')\n",
    "\n",
    "favorite_count10 = d.sort_values(by='Favorite Count').head(10).copy()\n",
    "\n",
    "#favorite_count10.to_csv('favorite_count.csv10')\n",
    "\n",
    "d['Retweet by Followers'] = d['Retweet Count'] / d['Followers Count']\n",
    "retweet_by_followers10 = d.sort_values(by='Retweet by Followers', ascending=False).head(10).copy()\n",
    "\n",
    "#retweet_by_followers10.to_csv('retweet_by_followers10.csv')\n",
    "\n",
    "d['Retweet by Friends'] = d['Retweet Count'] / d['Friends Count']\n",
    "retweet_by_friends10 = d.sort_values(by='Retweet by Friends', ascending=False).head(10).copy()\n",
    "\n",
    "#retweet_by_friends10.to_csv('retweet_by_friends10.csv')\n",
    "\n",
    "d['Favorite by Followers'] = d['Favorite Count'] / d['Followers Count']\n",
    "favorite_by_followers10  = d.sort_values(by='Favorite by Followers', ascending=False).head(10).copy()\n",
    "\n",
    "#favorite_by_followers10.to_csv('favorite_by_followers10.csv')\n",
    "\n",
    "d['Favorite by Friends'] = d['Favorite Count'] / d['Friends Count']\n",
    "favorite_by_friends10 = d.sort_values(by='Favorite by Friends', ascending=False).head(10).copy()\n",
    "\n",
    "#favorite_by_friends10.to_csv('favorite_by_friends10.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "NC1EAk0icV0w",
    "outputId": "da7efc9f-d8a8-4b93-d7dc-9317d4dfb2a2"
   },
   "outputs": [],
   "source": [
    "# Find the most common words \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.summarization import keywords\n",
    "import re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Choose a language\n",
    "\n",
    "stopwords_language = 'english' \n",
    "\n",
    "\n",
    "stop = stopwords.words(stopwords_language) + list(string.punctuation)\n",
    "\n",
    "stop = stop + ['http', 'https', 'amp'] \n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "combined_text = [w for w in d['Tweet Text']]\n",
    "\n",
    "combined_text = str(combined_text)\n",
    "\n",
    "tok_text = [w for w in word_tokenize(str(combined_text))]\n",
    "\n",
    "tok_text = [w.lower() for w in tok_text if w not in stop and len(w) >2]\n",
    "\n",
    "\n",
    "tok_text = [lemma.lemmatize(w) for w in tok_text]\n",
    "\n",
    "keys = keywords(str(tok_text), words=15).split('\\n')\n",
    "\n",
    "key_words = keys.copy()\n",
    "\n",
    "keys_score =  keywords(str(tok_text), scores = True, words=24)#.split('\\n')\n",
    "\n",
    "#print(keys_score)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant words \n",
    "\n",
    "remove = []\n",
    "\n",
    "clean_keys = [w for w in keys if w not in remove]\n",
    "clean_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "LukaiuT8r7y7",
    "outputId": "1a1b8567-1fb7-46ea-c452-ce3761c6a6ea"
   },
   "outputs": [],
   "source": [
    "# Produce a wordcloud \n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "stopwords_cloud = set(STOPWORDS)\n",
    "\n",
    "word_keys = re.sub(\"'\", \" \",str(keys))\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=20, stopwords = stopwords_cloud, background_color=\"white\").generate(word_keys)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-9D6pGCx2fM"
   },
   "outputs": [],
   "source": [
    "# Keywords by place\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "d['Tweet Text String'] =  d['Tweet Text'].str.lower()\n",
    "\n",
    "\n",
    "list_f = []\n",
    "\n",
    "list_location = []\n",
    "\n",
    "for ii in range(len(key_words)): \n",
    "      \n",
    "    location = []\n",
    "    for i in range(0, len(d)): \n",
    "\n",
    "        if clean_keys[ii] in d.iloc[i, -1]:\n",
    "            location.append(d.iloc[i, 5])\n",
    "    fig_loc = pd.Series(location)\n",
    "    fig_loc2 = fig_loc.value_counts()\n",
    "    fig_loc2[:10].sort_values().plot(title=clean_keys[ii], kind='barh')\n",
    "    fname = '{}.jpg'.format(clean_keys[ii])\n",
    "    plt.savefig(fname)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo \n",
    "# 1. clustering and extract key messages - then how they change over time - maybe art or nn\n",
    "# 2. sentiment analysis - nn \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "rixiWmH0dQeo",
    "outputId": "34398ef9-f388-44c4-dd79-e4e69455f520"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MCVfypNzz0Eo",
    "outputId": "2b7ee63d-a3f3-47c3-d8db-a7593efcff17"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoJHT-Ma5I-1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tweepy_cs1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
